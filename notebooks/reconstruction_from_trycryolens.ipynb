{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CryoLens Reconstruction from NDJSON Coordinates\n",
    "\n",
    "This notebook loads particle coordinates from an NDJSON file and generates reconstructions using CryoLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import copick\n",
    "\n",
    "from cryolens.utils.checkpoint_loading import load_vae_model\n",
    "from cryolens.inference.pipeline import InferencePipeline\n",
    "from cryolens.data.copick import extract_particles_from_tomogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "NDJSON_PATH = Path(\"/mnt/czi-sci-ai/imaging-models/kyle/git/czi-ai/cryolens/notebooks/try_cryolens_ribosome_results.ndjson\")\n",
    "CHECKPOINT_PATH = Path(\"/mnt/czi-sci-ai/imaging-models/cryolens/mlflow/outputs/alternating_curriculum/cryolens-sim-015/checkpoints/model_epoch_2600_train_loss_6.436.pt\")\n",
    "COPICK_CONFIG = Path(\"/mnt/czi-sci-ai/imaging-models/data/cryolens/mlc/copick_czcdp/ml_challenge_experimental_only.json\")\n",
    "STRUCTURES_DIR = Path(\"/mnt/czi-sci-ai/imaging-models/data/cryolens/mlc/structures/mrcs\")\n",
    "OUTPUT_DIR = Path(\"./reconstructions\")\n",
    "\n",
    "# Parameters\n",
    "VOXEL_SIZE = 10.0  # Angstroms\n",
    "BOX_SIZE = 48  # Voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NDJSON Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 coordinates\n",
      "Run name: TS_100_3\n",
      "Structure: ribosome\n"
     ]
    }
   ],
   "source": [
    "# Load coordinates from NDJSON\n",
    "coordinates = []\n",
    "with open(NDJSON_PATH) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        coordinates.append(data)\n",
    "\n",
    "print(f\"Loaded {len(coordinates)} coordinates\")\n",
    "print(f\"Run name: {coordinates[0]['run_name']}\")\n",
    "print(f\"Structure: {coordinates[0]['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Copick Project and Find Matching Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CopickRunMetaCDP(name='16463', portal_run_id=16463, portal_run_name='TS_5_4')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.runs[0].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for run: TS_100_3\n",
      "Found matching run: 17682\n"
     ]
    }
   ],
   "source": [
    "# Load Copick project\n",
    "root = copick.from_file(str(COPICK_CONFIG))\n",
    "\n",
    "# Extract run name from NDJSON (e.g., \"TS_100_3\")\n",
    "target_run_name = coordinates[0]['run_name']\n",
    "print(f\"Looking for run: {target_run_name}\")\n",
    "\n",
    "# Find matching run - the run name in Copick might have a different format\n",
    "# Try to match based on the tomogram identifier in the run's metadata\n",
    "matching_run = None\n",
    "for run in root.runs:\n",
    "    # Check if run name directly matches\n",
    "    if run.meta.portal_run_name == target_run_name:\n",
    "        matching_run = run\n",
    "        break\n",
    "\n",
    "if matching_run is None:\n",
    "    print(f\"Available runs: {[r.name for r in root.runs][:10]}\")\n",
    "    raise ValueError(f\"Could not find run matching '{target_run_name}'\")\n",
    "\n",
    "print(f\"Found matching run: {matching_run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tomogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using voxel spacing: 10.012Å\n",
      "Tomogram shape: (184, 630, 630)\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "# Find voxel spacing closest to target\n",
    "best_vs = None\n",
    "best_diff = float('inf')\n",
    "for vs in matching_run.voxel_spacings:\n",
    "    diff = abs(vs.voxel_size - VOXEL_SIZE)\n",
    "    if diff < best_diff:\n",
    "        best_diff = diff\n",
    "        best_vs = vs\n",
    "\n",
    "print(f\"Using voxel spacing: {best_vs.voxel_size}Å\")\n",
    "\n",
    "# Get tomogram (prefer denoised)\n",
    "tomograms = list(best_vs.tomograms)\n",
    "tomogram = None\n",
    "for tomo in tomograms:\n",
    "    if hasattr(tomo, 'tomo_type') and tomo.tomo_type == 'denoised':\n",
    "        tomogram = tomo\n",
    "        break\n",
    "if tomogram is None:\n",
    "    tomogram = tomograms[0]\n",
    "\n",
    "# Load tomogram data\n",
    "tomo_zarr = zarr.open(tomogram.zarr(), mode='r')\n",
    "for key in ['0', 's0', 'data']:\n",
    "    if key in tomo_zarr:\n",
    "        tomogram_data = np.array(tomo_zarr[key])\n",
    "        break\n",
    "else:\n",
    "    tomogram_data = np.array(tomo_zarr[list(tomo_zarr.keys())[0]])\n",
    "\n",
    "print(f\"Tomogram shape: {tomogram_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Particles at Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 10 particles...\n",
      "Extracted particles shape: (9, 48, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# Extract positions from NDJSON\n",
    "positions = []\n",
    "for coord in coordinates:\n",
    "    loc = coord['location']\n",
    "    positions.append((loc['x'], loc['y'], loc['z']))\n",
    "\n",
    "print(f\"Extracting {len(positions)} particles...\")\n",
    "\n",
    "# Extract particles\n",
    "particles = extract_particles_from_tomogram(\n",
    "    tomogram_data=tomogram_data,\n",
    "    positions=positions,\n",
    "    voxel_spacing=best_vs.voxel_size,\n",
    "    box_size=BOX_SIZE,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "particles = np.array(particles)\n",
    "print(f\"Extracted particles shape: {particles.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Generate Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 10:27:29 - cryolens.utils.checkpoint_loading - INFO - Removing 1 coordinate/renderer buffers for compatibility\n",
      "2025-10-23 10:27:29 - cryolens.utils.checkpoint_loading - INFO - Inferred from checkpoint: num_splats=768, latent_ratio=0.800\n",
      "2025-10-23 10:27:29 - cryolens.utils.checkpoint_loading - INFO - Loaded training parameters from: /mnt/czi-sci-ai/imaging-models/cryolens/mlflow/outputs/alternating_curriculum/cryolens-sim-015/training_params.json\n",
      "2025-10-23 10:27:29 - cryolens.utils.checkpoint_loading - INFO - Applied training parameters from config file\n",
      "2025-10-23 10:27:30 - cryolens.utils.checkpoint_loading - INFO - Model loaded successfully from /mnt/czi-sci-ai/imaging-models/cryolens/mlflow/outputs/alternating_curriculum/cryolens-sim-015/checkpoints/model_epoch_2600_train_loss_6.436.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model, config = load_vae_model(\n",
    "    CHECKPOINT_PATH,\n",
    "    device=device,\n",
    "    load_config=True,\n",
    "    strict_loading=False\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = InferencePipeline(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    normalization_method=config.get('normalization', 'z-score')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 particles...\n",
      "Generated 9 reconstructions\n",
      "Saved reconstruction to: reconstructions/TS_100_3_ribosome_reconstruction.mrc\n",
      "Saved embeddings: reconstructions/TS_100_3_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Generate reconstructions using InferencePipeline\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_reconstructions = []\n",
    "all_embeddings = []\n",
    "\n",
    "print(f\"Processing {len(particles)} particles...\")\n",
    "\n",
    "for i, particle in enumerate(particles):\n",
    "    # Use the pipeline to process each particle\n",
    "    result = pipeline.process_volume(\n",
    "        particle,\n",
    "        return_embeddings=True,\n",
    "        return_reconstruction=True\n",
    "    )\n",
    "    \n",
    "    all_reconstructions.append(result['reconstruction'])\n",
    "    all_embeddings.append(result['embeddings'])\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(particles)} particles\")\n",
    "\n",
    "print(f\"Generated {len(all_reconstructions)} reconstructions\")\n",
    "\n",
    "# Stack and average reconstructions\n",
    "reconstructions = np.array(all_reconstructions)\n",
    "avg_reconstruction = reconstructions.mean(axis=0)\n",
    "\n",
    "# Save as MRC\n",
    "import mrcfile\n",
    "output_path = OUTPUT_DIR / f\"{target_run_name}_ribosome_reconstruction.mrc\"\n",
    "with mrcfile.new(str(output_path), overwrite=True) as mrc:\n",
    "    mrc.set_data(avg_reconstruction.astype(np.float32))\n",
    "    mrc.voxel_size = best_vs.voxel_size\n",
    "\n",
    "print(f\"Saved reconstruction to: {output_path}\")\n",
    "\n",
    "# Also save embeddings for later analysis\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "np.save(OUTPUT_DIR / f\"{target_run_name}_embeddings.npy\", embeddings_array)\n",
    "print(f\"Saved embeddings: {OUTPUT_DIR / f'{target_run_name}_embeddings.npy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Visualize (Optional)\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show central slices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Handle different possible shapes\n",
    "if avg_reconstruction.ndim == 4:\n",
    "    vol = avg_reconstruction[0]  # Remove batch dimension\n",
    "elif avg_reconstruction.ndim == 3:\n",
    "    vol = avg_reconstruction\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected reconstruction shape: {avg_reconstruction.shape}\")\n",
    "\n",
    "center = vol.shape[0] // 2\n",
    "\n",
    "axes[0].imshow(vol[center, :, :], cmap='gray')\n",
    "axes[0].set_title('XY slice')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(vol[:, center, :], cmap='gray')\n",
    "axes[1].set_title('XZ slice')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(vol[:, :, center], cmap='gray')\n",
    "axes[2].set_title('YZ slice')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f\"{target_run_name}_slices.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved slices to: {OUTPUT_DIR / f'{target_run_name}_slices.png'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
